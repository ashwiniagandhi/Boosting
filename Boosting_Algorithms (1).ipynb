{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jflpv7PA3_5d"
   },
   "source": [
    "### Boosting Algorithms\n",
    "The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.\n",
    "Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
    "- Ada Boosting (Adaptive Boosting)\n",
    "- Gradient Tree Boosting\n",
    "- XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDiJCfYC3_5f"
   },
   "source": [
    "### How Boosting Algorithms works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlJokkb13_5h"
   },
   "source": [
    "Boosting combines weak learner using base learners to form a strong rule.To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "\n",
    "Here’s another question which might haunt you, ‘How do we choose different distribution for each round?’\n",
    "\n",
    "For choosing the right distribution, here are the following steps:\n",
    "\n",
    "**Step 1:** The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "**Step 2:** If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "**Step 3:** Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tz3RYEcy3_5j"
   },
   "source": [
    "### Ada Boosting (Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iTFbMgU3_5l"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FidR-my3_5n"
   },
   "source": [
    "**Box 1:** You can see that we have assigned equal weights to each data point and applied a decision stump to classify them as + (plus) or – (minus). The decision stump (D1) has generated vertical line at left side to classify the data points. We see that, this vertical line has incorrectly predicted three + (plus) as – (minus). In such case, we’ll assign higher weights to these three + (plus) and apply another decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o24CQURp3_5q"
   },
   "source": [
    "**Box 2:** Here, you can see that the size of three incorrectly predicted + (plus) is bigger as compared to rest of the data points. In this case, the second decision stump (D2) will try to predict them correctly. Now, a vertical line (D2) at right side of this box has classified three mis-classified + (plus) correctly. But again, it has caused mis-classification errors. This time with three -(minus). Again, we will assign higher weight to three – (minus) and apply another decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j33it2Vx3_5r"
   },
   "source": [
    "**Box 3:** Here, three – (minus) are given higher weights. A decision stump (D3) is applied to predict these mis-classified observation correctly. This time a horizontal line is generated to classify + (plus) and – (minus) based on higher weight of mis-classified observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jfwjwjo3_5s"
   },
   "source": [
    "**Box 4:** Here, we have combined D1, D2 and D3 to form a strong prediction having complex rule as compared to individual weak learner. You can see that this algorithm has classified these observation quite well as compared to any of individual weak learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yf2_GW23_5u"
   },
   "source": [
    "**AdaBoost (Adaptive Boosting) :** It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\n",
    "\n",
    "Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ggus6cd3_5v"
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e6n-IgGD3_5x"
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.ensemble import AdaBoostRegressor #For Regression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bLBt6km63_57",
    "outputId": "80a16e9e-af16-4e28-99d2-0c311c562fb5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hredata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4dc6445244b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reading the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hredata.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\logs\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\logs\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\logs\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\logs\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\logs\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hredata.csv'"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "df = pd.read_csv('hredata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNvFpaqp3_6H",
    "outputId": "efb346a0-ecb7-484a-a5e7-6f24775bf739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11557, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a58luONl3_6O",
    "outputId": "75736864-073a-4ccd-bc50-e1193c24bcd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['satisfaction_level', 'last_evaluation', 'number_of_projects',\n",
       "       'average_monthly_hours', 'years_at_company', 'work_accident', 'left',\n",
       "       'promotion_last_5years', 'department', 'salary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I75uQ5KL3_6f",
    "outputId": "0dc0835c-0aef-4f51-aca8-046b75248d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "satisfaction_level       0\n",
       "last_evaluation          1\n",
       "number_of_projects       1\n",
       "average_monthly_hours    1\n",
       "years_at_company         1\n",
       "work_accident            1\n",
       "left                     1\n",
       "promotion_last_5years    1\n",
       "department               1\n",
       "salary                   1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB-FNOmn3_6o",
    "outputId": "8b88d7ec-199f-4a61-d635-c80251a69aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11557 entries, 0 to 11556\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   satisfaction_level     11557 non-null  float64\n",
      " 1   last_evaluation        11556 non-null  float64\n",
      " 2   number_of_projects     11556 non-null  float64\n",
      " 3   average_monthly_hours  11556 non-null  float64\n",
      " 4   years_at_company       11556 non-null  float64\n",
      " 5   work_accident          11556 non-null  float64\n",
      " 6   left                   11556 non-null  float64\n",
      " 7   promotion_last_5years  11556 non-null  float64\n",
      " 8   department             11556 non-null  object \n",
      " 9   salary                 11556 non-null  object \n",
      "dtypes: float64(8), object(2)\n",
      "memory usage: 903.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abhdLGys3_6y"
   },
   "outputs": [],
   "source": [
    "df['last_evaluation'] = df['last_evaluation'].fillna(df['last_evaluation'].mean())\n",
    "df['number_of_projects'] = df['number_of_projects'].fillna(df['number_of_projects'].mean())\n",
    "df['average_monthly_hours'] = df['average_monthly_hours'].fillna(df['average_monthly_hours'].mean())\n",
    "df['years_at_company'] = df['years_at_company'].fillna(df['years_at_company'].mean())\n",
    "df['work_accident'] = df['work_accident'].fillna(df['work_accident'].mean())\n",
    "df['left'] = df['left'].fillna(df['left'].mode()[0])\n",
    "df['promotion_last_5years'] = df['promotion_last_5years'].fillna(df['promotion_last_5years'].mean())\n",
    "df['department'] = df['department'].fillna(df['department'].mode()[0])\n",
    "df['salary'] = df['salary'].fillna(df['salary'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W4Y7VfE3_66",
    "outputId": "76a71543-ec49-4cf2-ba84-cd4200f76075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       5571\n",
       "medium    5091\n",
       "high       895\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.salary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgT3Er9z3_7D",
    "outputId": "441ad3fd-1b11-4dcc-f0cb-3dc8abd659ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sales          3081\n",
       "technical      2227\n",
       "support        1790\n",
       "IT              933\n",
       "RandD           689\n",
       "product_mng     634\n",
       "marketing       632\n",
       "accounting      611\n",
       "hr              601\n",
       "management      359\n",
       "Name: department, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.department.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzaaIF2d3_7K"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "df['department'] = lb.fit_transform(df['department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAqD8OoH3_7Q",
    "outputId": "2208e668-d7ba-4091-d60e-d661569970f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    3081\n",
       "9    2227\n",
       "8    1790\n",
       "0     933\n",
       "1     689\n",
       "6     634\n",
       "5     632\n",
       "2     611\n",
       "3     601\n",
       "4     359\n",
       "Name: department, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.department.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "519ImE8M3_7X"
   },
   "outputs": [],
   "source": [
    "# separating the independent and dependent variables\n",
    "# independent variable\n",
    "X = df.drop('salary', axis=1)\n",
    "# dependent variable\n",
    "y = df['salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIkXdrH83_7f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_sc = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6-OE-a53_7k",
    "outputId": "8318f7dc-c088-4a6d-e868-265fa28ebe5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8089, 9) (3468, 9) (8089,) (3468,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_train_sc,y,test_size=0.3)\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skFx_8LM3_7q",
    "outputId": "d4a43399-a318-465f-9760-eda7fefb62fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=None,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will use decision tree as a base estimator, you can use any ML learner as base estimator if it accepts sample weight \n",
    "dt = DecisionTreeClassifier(class_weight=\"balanced\") \n",
    "clf = AdaBoostClassifier(n_estimators=100, base_estimator=dt,learning_rate=1)\n",
    "\n",
    "# training the model\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHXaGihe3_7y",
    "outputId": "f22f1f57-d110-4bef-948e-d589f2cd3375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on train data ['medium' 'low' 'low' ... 'low' 'medium' 'high']\n",
      "\n",
      "accuracy_score on train dataset :  0.9996291259735444\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the train dataset\n",
    "predict_train = clf.predict(x_train)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(y_train,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqX-rpbW3_75",
    "outputId": "d229a829-736e-4305-f046-636486122c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on test data ['medium' 'medium' 'low' ... 'medium' 'low' 'low']\n",
      "\n",
      "accuracy_score on test dataset :  0.47808535178777395\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = clf.predict(x_test)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV1pIqHg3_8C"
   },
   "source": [
    "We can tune the parameters to optimize the performance of algorithms, I’ve mentioned below the key parameters for tuning:\n",
    "\n",
    "**n_estimators:** It controls the number of weak learners.<br>\n",
    "**learning_rate:** Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.<br>\n",
    "**base_estimators:** It helps to specify different ML algorithm.<br>\n",
    "We can also tune the parameters of base learners to optimize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61TMCLj_3_8D"
   },
   "source": [
    "### Gradient Boosting\n",
    "A Gradient Boosting Machine or GBM combines the predictions from multiple decision trees to generate the final predictions. Keep in mind that all the weak learners in a gradient boosting machine are decision trees.But if we are using the same algorithm, then how is using a hundred decision trees better than using a single decision tree? How do different decision trees capture different signals/information from the data?\n",
    "\n",
    "Here is the trick – the nodes in every decision tree take a different subset of features for selecting the best split. This means that the individual trees aren’t all the same and hence they are able to capture different signals from the data.\n",
    "Additionally, each new tree takes into account the errors or mistakes made by the previous trees. So, every successive decision tree is built on the errors of the previous trees. This is how the trees in a gradient boosting machine algorithm are built sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPc6iALx3_8F"
   },
   "source": [
    "In gradient boosting, it trains many model sequentially. Each new model gradually minimizes the loss function (y = ax + b + e, e needs special attention as it is an error term) of the whole system using Gradient Descent method. The learning procedure consecutively fit new models to provide a more accurate estimate of the response variable.\n",
    "\n",
    "The principle idea behind this algorithm is to construct new base learners which can be maximally correlated with negative gradient of the loss function, associated with the whole ensemble.\n",
    "\n",
    "In Python Sklearn library, we use Gradient Tree Boosting or GBRT. It is a generalization of boosting to arbitrary differentiable loss functions. It can be used for both regression and classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2Sre0Qv3_8H"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7gLIuxM3_8P",
    "outputId": "cad2830d-b063-4f03-82c0-0cde345aa88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=1.0, loss='deviance', max_depth=1,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRpiNF383_8V",
    "outputId": "bfdbe839-4f70-4e3d-e3e8-a92dbbbcb7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on train data ['low' 'medium' 'medium' ... 'low' 'low' 'low']\n",
      "\n",
      "accuracy_score on train dataset :  0.5233032513289653\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the train dataset\n",
    "predict_train = clf.predict(x_train)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(y_train,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFyni9Zt3_8b",
    "outputId": "8670c472-e0a6-4105-86c1-2942ea41cdf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on test data ['low' 'low' 'low' ... 'medium' 'low' 'low']\n",
      "\n",
      "accuracy_score on test dataset :  0.48760092272203\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = clf.predict(x_test)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4satiu2L3_8j"
   },
   "source": [
    "**n_estimators:** It controls the number of weak learners.\n",
    "\n",
    "**learning_rate:** Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "**max_depth:** maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "We can tune loss function for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOhFdGB93_8l"
   },
   "source": [
    "### Extreme Gradient Boosting\n",
    "Extreme Gradient Boosting or XGBoost is another popular boosting algorithm. In fact, XGBoost is simply an improvised version of the GBM algorithm! The working procedure of XGBoost is the same as GBM. The trees in XGBoost are built sequentially, trying to correct the errors of the previous trees.\n",
    "\n",
    "But there are certain features that make XGBoost slightly better than GBM:\n",
    "\n",
    "- One of the most important points is that XGBM implements parallel preprocessing (at the node level) which makes it faster than GBM\n",
    "- XGBoost also includes a variety of regularization techniques that reduce overfitting and improve overall performance. You can select the regularization technique by setting the hyperparameters of the XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btCypAah3_8m",
    "outputId": "00c02978-b933-4d5d-f576-d9d2605de59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfVx4DRT3_8r",
    "outputId": "a440a6da-6ce7-41fc-a39a-732ef6c8dfd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on train data ['medium' 'low' 'low' ... 'low' 'medium' 'high']\n",
      "\n",
      "accuracy_score on train dataset :  0.8364445543330449\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(x_train)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(y_train,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H93UE3aZ3_8z",
    "outputId": "08fc3971-e612-4c3a-ad92-e6a80ffefb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target on test data ['medium' 'medium' 'low' ... 'low' 'low' 'low']\n",
      "\n",
      "accuracy_score on test dataset :  0.4749134948096886\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(x_test)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__SOq2E43_88"
   },
   "source": [
    "We can also tune the hyper parameters to optimize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQqukRjIZuoo"
   },
   "outputs": [],
   "source": [
    "UnSupervised Learning\n",
    "\n",
    "KMeans++\n",
    "\n",
    "KMeans(init=Kmean++)\n",
    "\n",
    "Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2SJ-t0abcQ6"
   },
   "outputs": [],
   "source": [
    "Supervised Learning\n",
    "\n",
    "Linear Regression\n",
    "Logistic regression\n",
    "KNN\n",
    "Naive Bayes\n",
    "Decision TRee\n",
    "Random Forest\n",
    "SVM\n",
    "Gradient Boosting,AdaBoost,XGBoost"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Boosting Algorithms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
